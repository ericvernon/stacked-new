{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-28T05:36:19.713764Z",
     "start_time": "2025-10-28T05:36:18.287857Z"
    }
   },
   "source": [
    "# Standard imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Load up the dataset, tune parameters for the base classifiers\n",
    "\n",
    "The ID refers to the ID in the UCI database -- i.e., https://archive.ics.uci.edu/dataset/863/maternal+health+risk"
   ],
   "id": "8c749c04fc54c46"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T05:40:17.799603Z",
     "start_time": "2025-10-28T05:40:16.680625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.data import load_dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "dataset_id = 863\n",
    "column_names = ['Age', 'SystolicBP', 'DiastolicBP', 'BS', 'BodyTemp', 'HeartRate']\n",
    "X, y = load_dataset(dataset_id)\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ],
   "id": "5b2dc5f3ae095a22",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tune XGB parameters\n",
    "Our goal isn't to get the best possible accuracy for each dataset, but we want to get \"close enough\" so that the results are meaningful.\n",
    "\n",
    "We use Optuna to tune the max_depth, min_child_weight, and gamma parameters over 500 trials.\n",
    "\n",
    "For each trial, we run a 5x3 stratified cross validation, and record the average accuracy. We will save the parameters which had the highest average accuracy and use those anytime we need to fit to the original, unmodified dataset (e.g., for training the base black box classifier)."
   ],
   "id": "f859adf8dad9ed44"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T06:04:48.288086Z",
     "start_time": "2025-10-28T05:41:19.313696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "def fn(trial):\n",
    "    max_depth = trial.suggest_int(\"max_depth\", low=4, high=128)\n",
    "    min_child_weight = trial.suggest_float(\"min_child_weight\", low=0, high=4)\n",
    "    gamma = trial.suggest_float(\"gamma\", low=0, high=4)\n",
    "    xgb = XGBClassifier(\n",
    "        max_depth=max_depth,\n",
    "        min_child_weight=min_child_weight,\n",
    "        gamma=gamma,\n",
    "        random_state=0,\n",
    "    )\n",
    "    cv = RepeatedStratifiedKFold(n_repeats=3, n_splits=5, random_state=0)\n",
    "    return cross_val_score(xgb, X, y, cv=cv).mean()\n",
    "\n",
    "sampler = TPESampler(seed=0)\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "study.optimize(fn, n_trials=500, n_jobs=1, show_progress_bar=True)\n",
    "\n",
    "optuna.visualization.plot_optimization_history(study).show()"
   ],
   "id": "173d683aafbce796",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cdb9af7061a44c309fb69d80dd60177e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "markers",
         "name": "Objective Value",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "y": [
          0.7370092181631954,
          0.7356728283665804,
          0.7133200019509341,
          0.741291518314393,
          0.8027491911752751,
          0.7274675250776309,
          0.7097107740330684,
          0.7254987075062186,
          0.7925409289697444,
          0.7586987270155586,
          0.8290380269554048,
          0.8352923962346973,
          0.7662683509730284,
          0.8425238582971598,
          0.7859809133622723,
          0.7478369019167928,
          0.7623177096034726,
          0.8481165357915101,
          0.8211578793347314,
          0.7774406997349982,
          0.8034011282901687,
          0.8477897543448927,
          0.8395665674941878,
          0.8116308182542392,
          0.8132614739306443,
          0.8296948414053227,
          0.8461428408850737,
          0.7869693865938319,
          0.7692175128842933,
          0.7402900388561023,
          0.7708579232307468,
          0.8346274528930725,
          0.8458128078817734,
          0.8458144336601148,
          0.7932123754247347,
          0.7475247524752474,
          0.728462501422556,
          0.8063649222065064,
          0.7899299289534866,
          0.8326570095433188,
          0.7412850152010275,
          0.8464712481100327,
          0.8162187647336162,
          0.8132647254873271,
          0.8487717244630867,
          0.7958412590027476,
          0.7590255084621762,
          0.8362711147962086,
          0.7284429920824594,
          0.8333121982148953,
          0.7915687135215984,
          0.8280544310588692,
          0.8418654180689005,
          0.8467980295566503,
          0.7968378611260143,
          0.8428457624087532,
          0.7718528995756718,
          0.7961680404493652,
          0.7909167764067049,
          0.716934107203824,
          0.7850005690224194,
          0.8398966004974882,
          0.8484449430164692,
          0.8418605407338763,
          0.8467964037783088,
          0.82345185257442,
          0.841543513957307,
          0.8283828382838283,
          0.8122778780341089,
          0.8448275862068966,
          0.8412037262839583,
          0.8411988489489343,
          0.8408850737290477,
          0.8487766017981109,
          0.8204945617714482,
          0.8438374871969956,
          0.821813068006308,
          0.8056999788648816,
          0.7268220910761026,
          0.8346274528930725,
          0.8277309011689346,
          0.8435123315287194,
          0.8464744996667155,
          0.8487717244630868,
          0.8405517891690646,
          0.8408850737290479,
          0.8300297517436472,
          0.817203986408493,
          0.8438407387536783,
          0.8435009510803297,
          0.8477881285665513,
          0.8441740233136613,
          0.8402120014957161,
          0.8448259604285552,
          0.849101757466387,
          0.8139150368238793,
          0.8435025768586711,
          0.8241151701377034,
          0.8264042660423677,
          0.8119543481441741,
          0.7504885463915849,
          0.8467947779999675,
          0.8290412785120875,
          0.8471264367816093,
          0.7780958884065747,
          0.8448210830935311,
          0.8145881090572111,
          0.8392251540424979,
          0.8063584190931409,
          0.7465395308003707,
          0.8365995220211676,
          0.8494285389130047,
          0.8352842673429904,
          0.8214781576679834,
          0.8484449430164691,
          0.8379082735859793,
          0.8323269765400184,
          0.8191793070932708,
          0.8208229689964068,
          0.8464761254450567,
          0.8310084703051586,
          0.8343006714464549,
          0.8471248110032678,
          0.8428555170788014,
          0.8007673673771317,
          0.8385764684842869,
          0.8349526085613487,
          0.8451559934318555,
          0.8280560568372108,
          0.8412167325106894,
          0.7402802841860541,
          0.8431855500821018,
          0.8471280625599505,
          0.8382448097026451,
          0.8382545643726934,
          0.8487749760197695,
          0.8395519354891153,
          0.8438374871969956,
          0.8073468923247005,
          0.847778373896503,
          0.8441626428652718,
          0.8481149100131689,
          0.8471264367816091,
          0.8494285389130046,
          0.8425206067404769,
          0.8471280625599505,
          0.8474515924498854,
          0.8356094230112666,
          0.8415353850656,
          0.8467931522216261,
          0.8431725438553708,
          0.8461428408850737,
          0.8477816254531857,
          0.8425271098538424,
          0.8425189809621355,
          0.8428587686354841,
          0.8408688159456339,
          0.8461379635500497,
          0.8421938252938594,
          0.8471248110032678,
          0.8481100326781447,
          0.8497618234729878,
          0.847129688338292,
          0.8425222325188184,
          0.8425271098538425,
          0.8481149100131689,
          0.849096880131363,
          0.8395551870457982,
          0.8425303614105252,
          0.8372547106927442,
          0.843510705750378,
          0.8484449430164692,
          0.8477881285665513,
          0.8490985059097044,
          0.8431806727470776,
          0.7221853712464843,
          0.7139833195142173,
          0.8464712481100326,
          0.8464712481100327,
          0.83856833959258,
          0.7580435383439822,
          0.8464696223316912,
          0.8405566665040888,
          0.8418621665122177,
          0.8484416914597864,
          0.8454795233217903,
          0.8454762717651076,
          0.841858914955535,
          0.847129688338292,
          0.8346242013363897,
          0.8412021005056171,
          0.8484433172381277,
          0.839896600497488,
          0.8471248110032679,
          0.8421889479588353,
          0.8467964037783088,
          0.8181908338617112,
          0.8362711147962085,
          0.8467980295566503,
          0.839234908712546,
          0.8438407387536784,
          0.8491001316880457,
          0.846801281113333,
          0.8441723975353199,
          0.8468029068916744,
          0.8418654180689005,
          0.8336389796615129,
          0.832000195093401,
          0.84778650278821,
          0.8461379635500496,
          0.8421921995155179,
          0.7935472857630592,
          0.8445024305386203,
          0.8412086036189826,
          0.850086979141264,
          0.8471296883382919,
          0.8490968801313629,
          0.7790794843031102,
          0.8392381602692288,
          0.8461379635500496,
          0.8448227088718724,
          0.8454811491001318,
          0.84778650278821,
          0.8458079305467493,
          0.8467996553349917,
          0.8500886049196053,
          0.828044676388821,
          0.8467947779999675,
          0.8444926758685721,
          0.846801281113333,
          0.8441658944219544,
          0.8451527418751729,
          0.8487700986847454,
          0.8491001316880457,
          0.8415402624006242,
          0.8402282592791298,
          0.8445024305386203,
          0.8474564697849095,
          0.8408769448373408,
          0.8487684729064039,
          0.8408834479507065,
          0.8418686696255833,
          0.847129688338292,
          0.85041213480954,
          0.8182005885317595,
          0.8484433172381277,
          0.8467931522216261,
          0.8484449430164691,
          0.849101757466387,
          0.8438391129753369,
          0.8454795233217903,
          0.836929555024468,
          0.8444975532035962,
          0.8474597213415922,
          0.8415337592872587,
          0.84646799655335,
          0.8415370108439414,
          0.8306833146368823,
          0.8477881285665512,
          0.8418702954039244,
          0.8369279292461265,
          0.8402152530523989,
          0.7314181664471866,
          0.84646799655335,
          0.8471313141166334,
          0.842849013965436,
          0.8491050090230698,
          0.8346274528930725,
          0.8467996553349917,
          0.8438358614186542,
          0.7116779658261394,
          0.8402282592791298,
          0.759028760018859,
          0.8454795233217903,
          0.8372612138061096,
          0.8421987026288834,
          0.8369246776894438,
          0.8484416914597864,
          0.8421987026288835,
          0.8471231852249265,
          0.7379749304979758,
          0.841530507730576,
          0.7517956721780552,
          0.8481149100131689,
          0.8402266335007884,
          0.8412102293973239,
          0.8451543676535141,
          0.846801281113333,
          0.8342990456681136,
          0.8431806727470776,
          0.8415418881789656,
          0.8448308377635793,
          0.8481165357915103,
          0.8438358614186542,
          0.7662618478596629,
          0.8326570095433189,
          0.8362678632395258,
          0.8425206067404769,
          0.8435074541936951,
          0.8464663707750085,
          0.8454860264351558,
          0.8490968801313629,
          0.8326553837649775,
          0.8395568128241396,
          0.8425157294054528,
          0.8428522655221187,
          0.7300964086556438,
          0.8461444666634151,
          0.8260807361524328,
          0.8418686696255833,
          0.8477848770098685,
          0.8428506397437773,
          0.8425271098538424,
          0.8405534149474061,
          0.8333089466582125,
          0.8448227088718723,
          0.8352875188996731,
          0.8497553203596222,
          0.8425189809621355,
          0.7360191191532947,
          0.8490985059097044,
          0.8454811491001316,
          0.8471296883382919,
          0.8421954510722007,
          0.8494269131346633,
          0.84646799655335,
          0.8464647449966672,
          0.8467980295566503,
          0.8412183582890307,
          0.8435107057503779,
          0.8310084703051586,
          0.8481149100131689,
          0.8461412151067323,
          0.8421905737371767,
          0.8415418881789656,
          0.8477783738965029,
          0.8339690126648133,
          0.8484433172381277,
          0.821809816449625,
          0.8441707717569786,
          0.8481132842348275,
          0.8428555170788014,
          0.8487684729064038,
          0.8412069778406411,
          0.846801281113333,
          0.8431757954120536,
          0.8461379635500496,
          0.8405387829423336,
          0.8471280625599505,
          0.8467947779999675,
          0.7679022582061161,
          0.8412037262839583,
          0.8425254840755012,
          0.8428473881870945,
          0.7185793948853014,
          0.8464696223316912,
          0.8477865027882099,
          0.8441658944219544,
          0.8438391129753369,
          0.8487717244630868,
          0.8260758588174089,
          0.8392251540424979,
          0.8451527418751728,
          0.836921426132761,
          0.8464647449966672,
          0.847129688338292,
          0.8392332829342046,
          0.8418605407338763,
          0.725828740509519,
          0.8454811491001316,
          0.8425287356321839,
          0.785659009250679,
          0.729431465314019,
          0.8454827748784731,
          0.8494236615779804,
          0.8336406054398542,
          0.839896600497488,
          0.843182298525419,
          0.8431774211903948,
          0.8487700986847454,
          0.8490952543530216,
          0.8346242013363897,
          0.845482774878473,
          0.8073517696597245,
          0.8444926758685721,
          0.8461395893283911,
          0.8342974198897721,
          0.8458128078817733,
          0.8227901607894778,
          0.8421921995155179,
          0.8428571428571427,
          0.8451592449885383,
          0.8362694890178672,
          0.8428538913004602,
          0.849425287356322,
          0.841543513957307,
          0.8402217561657643,
          0.8422003284072249,
          0.8412134809540066,
          0.8306751857451754,
          0.8421938252938594,
          0.8497618234729878,
          0.8484433172381277,
          0.8392397860475703,
          0.8411988489489344,
          0.8444991789819375,
          0.8441740233136614,
          0.8467947779999675,
          0.8487717244630867,
          0.8435058284153539,
          0.8356126745679493,
          0.8405534149474061,
          0.84778650278821,
          0.8438391129753369,
          0.8454811491001318,
          0.8481132842348275,
          0.8435090799720366,
          0.8415256303955518,
          0.8471248110032678,
          0.8454844006568145,
          0.840225007722447,
          0.8497585719163051,
          0.8454811491001316,
          0.8448243346502139,
          0.8425157294054528,
          0.8441675202002958,
          0.8405501633907233,
          0.8428538913004602,
          0.8418572891771936,
          0.8467980295566502,
          0.8477865027882099,
          0.8418702954039247,
          0.8477848770098684,
          0.8346274528930725,
          0.8438358614186542,
          0.8474580955632509,
          0.8040611942967696,
          0.8336373538831715,
          0.8418751727389486,
          0.8461379635500496,
          0.8448259604285551,
          0.8329886683249605,
          0.8421954510722007,
          0.8425222325188184,
          0.8431822985254189,
          0.8487684729064039,
          0.845482774878473,
          0.8395681932725292,
          0.822138223674584,
          0.8421938252938594,
          0.8435107057503779,
          0.8448227088718724,
          0.8490985059097044,
          0.840886699507389,
          0.7616478889268238,
          0.7731649026971664,
          0.8458144336601148,
          0.7518005495130793,
          0.8389016241525631,
          0.8454844006568145,
          0.782040026662765,
          0.8408834479507064,
          0.7435773626623746,
          0.8415451397356485,
          0.8474580955632509,
          0.8412134809540067,
          0.8339738899998373,
          0.8467996553349917,
          0.8431806727470776,
          0.8385829715976524,
          0.8474515924498854,
          0.8497569461379636,
          0.8464696223316913,
          0.8435090799720366,
          0.8428555170788015,
          0.8458111821034321,
          0.8339625095514478,
          0.8474597213415923,
          0.8444975532035962,
          0.8421938252938593,
          0.8477799996748444,
          0.8474548440065681,
          0.8339738899998373,
          0.8418670438472418,
          0.7991253312523371,
          0.7307646035539516,
          0.846796403778309,
          0.7379781820546587,
          0.8487733502414281,
          0.8346225755580484,
          0.8431822985254189,
          0.8497553203596222,
          0.8461379635500496,
          0.8474515924498854,
          0.8435058284153538,
          0.8471231852249265,
          0.8352842673429904,
          0.8464663707750085,
          0.8471231852249265,
          0.846144466663415
         ],
         "type": "scatter"
        },
        {
         "mode": "lines",
         "name": "Best Value",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "y": [
          0.7370092181631954,
          0.7370092181631954,
          0.7370092181631954,
          0.741291518314393,
          0.8027491911752751,
          0.8027491911752751,
          0.8027491911752751,
          0.8027491911752751,
          0.8027491911752751,
          0.8027491911752751,
          0.8290380269554048,
          0.8352923962346973,
          0.8352923962346973,
          0.8425238582971598,
          0.8425238582971598,
          0.8425238582971598,
          0.8425238582971598,
          0.8481165357915101,
          0.8481165357915101,
          0.8481165357915101,
          0.8481165357915101,
          0.8481165357915101,
          0.8481165357915101,
          0.8481165357915101,
          0.8481165357915101,
          0.8481165357915101,
          0.8481165357915101,
          0.8481165357915101,
          0.8481165357915101,
          0.8481165357915101,
          0.8481165357915101,
          0.8481165357915101,
          0.8481165357915101,
          0.8481165357915101,
          0.8481165357915101,
          0.8481165357915101,
          0.8481165357915101,
          0.8481165357915101,
          0.8481165357915101,
          0.8481165357915101,
          0.8481165357915101,
          0.8481165357915101,
          0.8481165357915101,
          0.8481165357915101,
          0.8487717244630867,
          0.8487717244630867,
          0.8487717244630867,
          0.8487717244630867,
          0.8487717244630867,
          0.8487717244630867,
          0.8487717244630867,
          0.8487717244630867,
          0.8487717244630867,
          0.8487717244630867,
          0.8487717244630867,
          0.8487717244630867,
          0.8487717244630867,
          0.8487717244630867,
          0.8487717244630867,
          0.8487717244630867,
          0.8487717244630867,
          0.8487717244630867,
          0.8487717244630867,
          0.8487717244630867,
          0.8487717244630867,
          0.8487717244630867,
          0.8487717244630867,
          0.8487717244630867,
          0.8487717244630867,
          0.8487717244630867,
          0.8487717244630867,
          0.8487717244630867,
          0.8487717244630867,
          0.8487766017981109,
          0.8487766017981109,
          0.8487766017981109,
          0.8487766017981109,
          0.8487766017981109,
          0.8487766017981109,
          0.8487766017981109,
          0.8487766017981109,
          0.8487766017981109,
          0.8487766017981109,
          0.8487766017981109,
          0.8487766017981109,
          0.8487766017981109,
          0.8487766017981109,
          0.8487766017981109,
          0.8487766017981109,
          0.8487766017981109,
          0.8487766017981109,
          0.8487766017981109,
          0.8487766017981109,
          0.8487766017981109,
          0.849101757466387,
          0.849101757466387,
          0.849101757466387,
          0.849101757466387,
          0.849101757466387,
          0.849101757466387,
          0.849101757466387,
          0.849101757466387,
          0.849101757466387,
          0.849101757466387,
          0.849101757466387,
          0.849101757466387,
          0.849101757466387,
          0.849101757466387,
          0.849101757466387,
          0.849101757466387,
          0.849101757466387,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8494285389130047,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.8497618234729878,
          0.850086979141264,
          0.850086979141264,
          0.850086979141264,
          0.850086979141264,
          0.850086979141264,
          0.850086979141264,
          0.850086979141264,
          0.850086979141264,
          0.850086979141264,
          0.850086979141264,
          0.850086979141264,
          0.8500886049196053,
          0.8500886049196053,
          0.8500886049196053,
          0.8500886049196053,
          0.8500886049196053,
          0.8500886049196053,
          0.8500886049196053,
          0.8500886049196053,
          0.8500886049196053,
          0.8500886049196053,
          0.8500886049196053,
          0.8500886049196053,
          0.8500886049196053,
          0.8500886049196053,
          0.8500886049196053,
          0.8500886049196053,
          0.8500886049196053,
          0.8500886049196053,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954,
          0.85041213480954
         ],
         "type": "scatter"
        },
        {
         "marker": {
          "color": "#cccccc"
         },
         "mode": "markers",
         "name": "Infeasible Trial",
         "showlegend": false,
         "x": [],
         "y": [],
         "type": "scatter"
        }
       ],
       "layout": {
        "title": {
         "text": "Optimization History Plot"
        },
        "xaxis": {
         "title": {
          "text": "Trial"
         }
        },
        "yaxis": {
         "title": {
          "text": "Objective Value"
         }
        },
        "template": {
         "data": {
          "histogram2dcontour": [
           {
            "type": "histogram2dcontour",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "choropleth": [
           {
            "type": "choropleth",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           }
          ],
          "histogram2d": [
           {
            "type": "histogram2d",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "heatmap": [
           {
            "type": "heatmap",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "contourcarpet": [
           {
            "type": "contourcarpet",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           }
          ],
          "contour": [
           {
            "type": "contour",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "surface": [
           {
            "type": "surface",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "mesh3d": [
           {
            "type": "mesh3d",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "parcoords": [
           {
            "type": "parcoords",
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatterpolargl": [
           {
            "type": "scatterpolargl",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "scattergeo": [
           {
            "type": "scattergeo",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatterpolar": [
           {
            "type": "scatterpolar",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "scattergl": [
           {
            "type": "scattergl",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatter3d": [
           {
            "type": "scatter3d",
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scattermap": [
           {
            "type": "scattermap",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scattermapbox": [
           {
            "type": "scattermapbox",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatterternary": [
           {
            "type": "scatterternary",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scattercarpet": [
           {
            "type": "scattercarpet",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ]
         },
         "layout": {
          "autotypenumbers": "strict",
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "hovermode": "closest",
          "hoverlabel": {
           "align": "left"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "bgcolor": "#E5ECF6",
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "ternary": {
           "bgcolor": "#E5ECF6",
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "sequential": [
            [
             0.0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1.0,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0.0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1.0,
             "#f0f921"
            ]
           ],
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ]
          },
          "xaxis": {
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "automargin": true,
           "zerolinewidth": 2
          },
          "yaxis": {
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "automargin": true,
           "zerolinewidth": 2
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white",
            "gridwidth": 2
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white",
            "gridwidth": 2
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white",
            "gridwidth": 2
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "geo": {
           "bgcolor": "white",
           "landcolor": "#E5ECF6",
           "subunitcolor": "white",
           "showland": true,
           "showlakes": true,
           "lakecolor": "white"
          },
          "title": {
           "x": 0.05
          },
          "mapbox": {
           "style": "light"
          }
         }
        }
       },
       "config": {
        "plotlyServerURL": "https://plot.ly"
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T06:07:09.601602Z",
     "start_time": "2025-10-28T06:07:09.595699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "xgb_params = study.best_params\n",
    "xgb_params"
   ],
   "id": "e44aeb377cb88a03",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 39,\n",
       " 'min_child_weight': 0.31366694223668196,\n",
       " 'gamma': 0.0011806553353583718}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "With default parameters, XGBoost scores ~84.187% accuracy.\n",
    "\n",
    "On Trial 17, Optuna found parameters with 84.811% accuracy. "
   ],
   "id": "28133c5588d88e2f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T06:08:09.299169Z",
     "start_time": "2025-10-28T06:08:04.932056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "xgb_default = XGBClassifier()\n",
    "xgb_default.fit(X, y)\n",
    "cross_val_score(xgb_default, X, y, cv=RepeatedStratifiedKFold(n_repeats=3, n_splits=5, random_state=0)).mean()"
   ],
   "id": "8e6ca7c728246024",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8418702954039247)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Train the glass box + black box models\n",
    "- Glass box: A decision tree with up to depth 4\n",
    "   - Under the hood, we use sklearn's grid search to find which `max_depth` parameter in the range [1, 4] performs best under 5x3 CV\n",
    "  - Usually (but not always) setting max_depth=4 gives the best results, including for this dataset\n",
    "- Black box: XGBoost classifier, using the hyperparameters tuned via Optuna"
   ],
   "id": "4b596c4f58a6091f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T06:12:06.572562Z",
     "start_time": "2025-10-28T06:12:05.907531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from functools import partial\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.models import tuned_decision_tree_classifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "glass_box_fn = partial(tuned_decision_tree_classifier, max_allowed_depth=4)\n",
    "glass_box = glass_box_fn()\n",
    "glass_box.fit(X_train, y_train)\n",
    "print('Glass box - Train accuracy: ', glass_box.score(X_train, y_train))\n",
    "print('Glass box - Test accuracy:  ', glass_box.score(X_test, y_test))\n",
    "\n",
    "black_box_fn = partial(XGBClassifier, random_state=0, **xgb_params)\n",
    "black_box = black_box_fn()\n",
    "black_box.fit(X_train, y_train)\n",
    "\n",
    "print('Black box - Train accuracy: ', black_box.score(X_train, y_train))\n",
    "print('Black box - Test accuracy:  ', black_box.score(X_test, y_test))"
   ],
   "id": "a3b13b45af4275bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glass box - Train accuracy:  0.7003699136868065\n",
      "Glass box - Test accuracy:   0.6699507389162561\n",
      "Black box - Train accuracy:  0.9334155363748459\n",
      "Black box - Test accuracy:   0.8226600985221675\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Collect \"difficult\" training patterns\n",
    "We want to identify patterns that are,\n",
    "- Classified correctly by the decision tree (\"easy\")\n",
    "- Classified correctly by the random forest, but not the decision tree (\"hard\")\n",
    "- Not correctly classified by either (\"very hard\")\n",
    "    - This also includes the handful of patterns the decision tree gets right, but not XGBoost\n",
    "\n",
    "We perform stratified k-fold (4-fold) cross validation on the training set -- i.e. it is split into sub-training (75%) and calibration (25%) data 4 times, with each pattern appearing in the calibration set once. Then, we collate all the patterns which were misclassified when they appeared in the calibration set.\n",
    "\n",
    "We should expect that this should yield *roughly* (1 - (Testing Accuracy))% of the training set.\n",
    "\n",
    "e.g. The testing accuracy of the glass box is ~67%, so we can expect ~33% of the training set to get misclassified during the CV.  "
   ],
   "id": "9f85ddf48f1e436f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T06:12:40.350278Z",
     "start_time": "2025-10-28T06:12:25.650669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.experiment import collect_wrong_indices\n",
    "\n",
    "glass_wrong_idx = collect_wrong_indices(glass_box_fn, X_train, y_train)\n",
    "print('Ratio of indices \"difficult\" for glass box: ', len(glass_wrong_idx) / len(X_train))"
   ],
   "id": "39a9d532d4cbc99a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of indices \"difficult\" for glass box:  0.5573366214549939\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T06:13:09.681771Z",
     "start_time": "2025-10-28T06:12:40.351790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "black_wrong_idx = collect_wrong_indices(black_box_fn, X_train, y_train)\n",
    "print('Ratio of indices \"difficult\" for black box: ', len(black_wrong_idx) / len(X_train))"
   ],
   "id": "8d4919a12577d168",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of indices \"difficult\" for black box:  0.3785450061652281\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, sort these patterns into easy/hard/very hard difficulties.",
   "id": "ae9f79a2c4f7f5ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.experiment import get_ternary_grader_data\n",
    "\n",
    "grader_x, grader_y = get_ternary_grader_data(glass_wrong_idx, black_wrong_idx, X_train, skip_oversampling=True)\n",
    "np.bincount(grader_y) / len(grader_y)"
   ],
   "id": "8f189f1452df3045",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "66.2% of the patterns are \"easy\", 21.5% \"hard\", and 12.2% \"very hard\".\n",
    "\n",
    "This kind of imbalanced dataset is difficult to learn from, *especially* when using a shallow decision tree.\n",
    "\n",
    "We oversample the minority classes using SMOTE in order to get a balanced training set for the grader."
   ],
   "id": "b01dcf12b9971357"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "grader_x, grader_y = get_ternary_grader_data(glass_wrong_idx, black_wrong_idx, X_train, skip_oversampling=False)\n",
    "np.bincount(grader_y) / len(grader_y)"
   ],
   "id": "92323a8e3446176a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "grader = glass_box_fn()\n",
    "grader.fit(grader_x, grader_y);"
   ],
   "id": "f4349856d5d6df08",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(le.inverse_transform([0, 1, 2]))",
   "id": "eb87695e18f690b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## The decision tree for the grader model",
   "id": "bf79862740f783ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.tree import plot_tree\n",
    "fig, ax = plt.subplots(figsize=(24, 8))\n",
    "plot_tree(grader.best_estimator_, feature_names=column_names, class_names=['Glass Box', 'Black Box', 'Reject'],\n",
    "          impurity=False,\n",
    "          ax=ax, fontsize=8);\n",
    "plt.show()"
   ],
   "id": "ea22f2b29b8f1e13",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## The glass box decision tree",
   "id": "20262ad52baaefdc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "plot_tree(glass_box.best_estimator_, feature_names=column_names, class_names=le.inverse_transform(np.arange(3)), ax=ax, fontsize=8, impurity=False)\n",
    "plt.show()"
   ],
   "id": "3ab89bf05edbe16c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.reporting import parse_results_df, results_dict_to_text\n",
    "df_train = pd.DataFrame(data={\n",
    "    'y_truth': y_train,\n",
    "    'y_glass': glass_box.predict(X_train),\n",
    "    'y_black': black_box.predict(X_train),\n",
    "    'y_grader': grader.predict(X_train),\n",
    "})\n",
    "df_test = pd.DataFrame(data={\n",
    "    'y_truth': y_test,\n",
    "    'y_glass': glass_box.predict(X_test),\n",
    "    'y_black': black_box.predict(X_test),\n",
    "    'y_grader': grader.predict(X_test),\n",
    "})"
   ],
   "id": "1eba9ad8a907bc79",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print('---TRAIN---')\n",
    "print(results_dict_to_text(parse_results_df(df_train)))"
   ],
   "id": "e72fcf9875ec5263",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print('---TEST---')\n",
    "print(results_dict_to_text(parse_results_df(df_test)))"
   ],
   "id": "7343a97aa5dd473",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "TRAINING_SET, TESTING_SET = 0, 1\n",
    "DIFF_EASY, DIFF_HARD, DIFF_V_HARD = 0, 1, 2\n",
    "test_grader_max_depth = 20"
   ],
   "id": "c08745ca31d9cb62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "# \n",
    "# \n",
    "# difficulty_distribution = np.empty(shape=(50, test_grader_max_depth + 1, 2, 3)) ## split idx, grader depth, train/test, easy/hard/very hard\n",
    "# final_accuracy_scores = np.empty(shape=(50, test_grader_max_depth + 1, 2))      ## split idx, grader depth, train/test\n",
    "# glass_box_accuracy = np.empty(shape=(50, test_grader_max_depth + 1,2, 3))       ## split idx, grader depth, train/test, easy/hard/very hard\n",
    "# black_box_accuracy = np.empty(shape=(50, test_grader_max_depth + 1,2, 3))       ## split idx, grader depth, train/test, easy/hard/very hard\n",
    "# \n",
    "# rskf = RepeatedStratifiedKFold(n_repeats=5, n_splits=10, random_state=0)\n",
    "# for data_split_idx, (train_index, test_index) in enumerate(rskf.split(X, y)):\n",
    "#     X_train, X_test = X[train_index], X[test_index]\n",
    "#     y_train, y_test = y[train_index], y[test_index]\n",
    "# \n",
    "#     glass_box = glass_box_fn()\n",
    "#     glass_box.fit(X_train, y_train)\n",
    "#     \n",
    "#     black_box = black_box_fn()\n",
    "#     black_box.fit(X_train, y_train)\n",
    "# \n",
    "#     for i in range(2, test_grader_max_depth + 1):\n",
    "#         grader_fn = partial(tuned_decision_tree_classifier, max_depth=i)\n",
    "#         grader = grader_fn()\n",
    "#         grader.fit(grader_x, grader_y)\n",
    "#         df_train = pd.DataFrame(data={\n",
    "#             'y_truth': y_train,\n",
    "#             'y_glass': glass_box.predict(X_train),\n",
    "#             'y_black': black_box.predict(X_train),\n",
    "#             'y_grader': grader.predict(X_train),\n",
    "#         })\n",
    "#         results_train = parse_results_df(df_train)\n",
    "#         difficulty_distribution[data_split_idx, i, TRAINING_SET, 0] = results_train['n_easy']\n",
    "#         difficulty_distribution[data_split_idx, i, TRAINING_SET, 1] = results_train['n_hard']\n",
    "#         difficulty_distribution[data_split_idx, i, TRAINING_SET, 2] = results_train['n_very_hard']\n",
    "#         final_accuracy_scores[data_split_idx, i, TRAINING_SET] = results_train['hybrid_n_correct_total'] / (results_train['n_total'] - results_train['n_very_hard'])\n",
    "#         glass_box_accuracy[data_split_idx, i, TRAINING_SET, 0] = results_train['glass_n_correct_easy'] / results_train['n_easy']\n",
    "#         glass_box_accuracy[data_split_idx, i, TRAINING_SET, 1] = results_train['glass_n_correct_hard'] / results_train['n_hard']\n",
    "#         glass_box_accuracy[data_split_idx, i, TRAINING_SET, 2] = results_train['glass_n_correct_very_hard'] / results_train['n_very_hard']\n",
    "#         black_box_accuracy[data_split_idx, i, TRAINING_SET, 0] = results_train['black_n_correct_easy'] / results_train['n_easy']\n",
    "#         black_box_accuracy[data_split_idx, i, TRAINING_SET, 1] = results_train['black_n_correct_hard'] / results_train['n_hard']\n",
    "#         black_box_accuracy[data_split_idx, i, TRAINING_SET, 2] = results_train['black_n_correct_very_hard'] / results_train['n_very_hard']\n",
    "#     \n",
    "#         df_test = pd.DataFrame(data={\n",
    "#             'y_truth': y_test,\n",
    "#             'y_glass': glass_box.predict(X_test),\n",
    "#             'y_black': black_box.predict(X_test),\n",
    "#             'y_grader': grader.predict(X_test),\n",
    "#         })\n",
    "#         results_test = parse_results_df(df_test)\n",
    "#         difficulty_distribution[data_split_idx, i, TESTING_SET, 0] = results_test['n_easy']\n",
    "#         difficulty_distribution[data_split_idx, i, TESTING_SET, 1] = results_test['n_hard']\n",
    "#         difficulty_distribution[data_split_idx, i, TESTING_SET, 2] = results_test['n_very_hard']\n",
    "#         final_accuracy_scores[data_split_idx, i, TESTING_SET] = results_test['hybrid_n_correct_total'] / (results_test['n_total'] - results_test['n_very_hard'])\n",
    "#         glass_box_accuracy[data_split_idx, i, TESTING_SET, 0] = results_test['glass_n_correct_easy'] / results_test['n_easy']\n",
    "#         glass_box_accuracy[data_split_idx, i, TESTING_SET, 1] = results_test['glass_n_correct_hard'] / results_test['n_hard']\n",
    "#         glass_box_accuracy[data_split_idx, i, TESTING_SET, 2] = results_test['glass_n_correct_very_hard'] / results_test['n_very_hard']\n",
    "#         black_box_accuracy[data_split_idx, i, TESTING_SET, 0] = results_test['black_n_correct_easy'] / results_test['n_easy']\n",
    "#         black_box_accuracy[data_split_idx, i, TESTING_SET, 1] = results_test['black_n_correct_hard'] / results_test['n_hard']\n",
    "#         black_box_accuracy[data_split_idx, i, TESTING_SET, 2] = results_test['black_n_correct_very_hard'] / results_test['n_very_hard']\n",
    "#         \n",
    "# difficulty_distribution.shape"
   ],
   "id": "dc34c52cfe59a6b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# np.save(\"difficulty_distribution.npy\", difficulty_distribution)\n",
    "# np.save(\"final_accuracy_scores.npy\", final_accuracy_scores)\n",
    "# np.save(\"glass_box_accuracy.npy\", glass_box_accuracy)\n",
    "# np.save(\"black_box_accuracy.npy\", black_box_accuracy)"
   ],
   "id": "2e134c4f82c8313f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "difficulty_distribution = np.load(\"difficulty_distribution.npy\")\n",
    "final_accuracy_scores = np.load(\"final_accuracy_scores.npy\")\n",
    "glass_box_accuracy = np.load(\"glass_box_accuracy.npy\")\n",
    "black_box_accuracy = np.load(\"black_box_accuracy.npy\")"
   ],
   "id": "6310bd2f03d2693b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 8), ncols=2)\n",
    "fig.suptitle('Effects of Increased Grader Depth (Testing Set)', fontsize=32)\n",
    "\n",
    "final_accuracy_mean = final_accuracy_scores.mean(axis=0)\n",
    "difficulty_ratios_test = (difficulty_distribution.mean(axis=0) / (X.shape[0] * 0.1))[:, TESTING_SET, :]\n",
    "\n",
    "### Left hand side\n",
    "categories = np.arange(test_grader_max_depth + 1)\n",
    "easy = difficulty_ratios_test[2:, 0]\n",
    "hard = difficulty_ratios_test[2:, 1]\n",
    "vhard = difficulty_ratios_test[2:, 2]\n",
    "x = np.arange(2, test_grader_max_depth + 1)\n",
    "\n",
    "ax[0].bar(x, easy, label='\"Easy\"')\n",
    "ax[0].bar(x, hard, bottom=easy, label='\"Hard\"')\n",
    "ax[0].bar(x, vhard, bottom=np.array(easy) + np.array(hard), label='\"Very Hard\"')\n",
    "ax[0].legend(fontsize=18, framealpha=0.95, shadow=True, loc='lower right')\n",
    "ax[0].set_title('Difficulty Distribution', fontsize=24)\n",
    "\n",
    "ax[0].set_xlim(1, test_grader_max_depth + 1)\n",
    "ax[0].set_xticks(np.arange(2, test_grader_max_depth + 1), np.arange(2, test_grader_max_depth + 1))\n",
    "ax[0].set_xlabel('Grader Depth', fontsize=18)\n",
    "\n",
    "ax[0].set_yticks(np.linspace(0, 1.0, 5), np.linspace(0.0, 1.0, 5))\n",
    "ax[0].set_ylabel('Ratio of Patterns', fontsize=18)\n",
    "\n",
    "### Right hand side\n",
    "ax[1].plot(np.arange(2, test_grader_max_depth + 1), final_accuracy_mean[2:, TESTING_SET], linewidth=8, color='k')\n",
    "\n",
    "ax[1].set_xlim(1, test_grader_max_depth + 1)\n",
    "ax[1].set_xticks(np.arange(2, test_grader_max_depth + 1), np.arange(2, test_grader_max_depth + 1))\n",
    "ax[1].set_xlabel('Grader Depth', fontsize=18)\n",
    "\n",
    "ax[1].set_ylim(0.6, 1.0)\n",
    "ax[1].grid()\n",
    "ax[1].set_yticks(np.linspace(0.6, 1.0, 9), np.linspace(0.6, 1.0, 9))\n",
    "ax[1].set_ylabel('Accuracy', fontsize=18)\n",
    "ax[1].set_title('Final Accuracy', fontsize=24)\n",
    "\n",
    "plt.show()"
   ],
   "id": "d33d1522b457d4ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 8), ncols=2)\n",
    "fig.suptitle('Effects of Increased Grader Depth (Training Set)', fontsize=32)\n",
    "\n",
    "final_accuracy_mean = final_accuracy_scores.mean(axis=0)\n",
    "difficulty_ratios_train = (difficulty_distribution.mean(axis=0) / (X.shape[0] * 0.9))[:, TRAINING_SET, :]\n",
    "\n",
    "### Left hand side\n",
    "categories = np.arange(test_grader_max_depth + 1)\n",
    "easy = difficulty_ratios_train[2:, 0]\n",
    "hard = difficulty_ratios_train[2:, 1]\n",
    "vhard = difficulty_ratios_train[2:, 2]\n",
    "x = np.arange(2, test_grader_max_depth + 1)\n",
    "\n",
    "ax[0].bar(x, easy, label='\"Easy\"')\n",
    "ax[0].bar(x, hard, bottom=easy, label='\"Hard\"')\n",
    "ax[0].bar(x, vhard, bottom=np.array(easy) + np.array(hard), label='\"Very Hard\"')\n",
    "ax[0].legend(fontsize=18, framealpha=0.95, shadow=True, loc='lower right')\n",
    "ax[0].set_title('Difficulty Distribution', fontsize=24)\n",
    "\n",
    "ax[0].set_xlim(1, test_grader_max_depth + 1)\n",
    "ax[0].set_xticks(np.arange(2, test_grader_max_depth + 1), np.arange(2, test_grader_max_depth + 1))\n",
    "ax[0].set_xlabel('Grader Depth', fontsize=18)\n",
    "\n",
    "ax[0].set_yticks(np.linspace(0, 1.0, 5), np.linspace(0.0, 1.0, 5))\n",
    "ax[0].set_ylabel('Ratio of Patterns', fontsize=18)\n",
    "\n",
    "### Right hand side\n",
    "ax[1].plot(np.arange(2, test_grader_max_depth + 1), final_accuracy_mean[2:, TRAINING_SET], linewidth=8, color='k')\n",
    "\n",
    "ax[1].set_xlim(1, test_grader_max_depth + 1)\n",
    "ax[1].set_xticks(np.arange(2, test_grader_max_depth + 1), np.arange(2, test_grader_max_depth + 1))\n",
    "ax[1].set_xlabel('Grader Depth', fontsize=18)\n",
    "\n",
    "ax[1].set_ylim(0.6, 1.0)\n",
    "ax[1].grid()\n",
    "ax[1].set_yticks(np.linspace(0.6, 1.0, 9), np.linspace(0.6, 1.0, 9))\n",
    "ax[1].set_ylabel('Accuracy', fontsize=18)\n",
    "ax[1].set_title('Final Accuracy', fontsize=24)\n",
    "\n",
    "plt.show()"
   ],
   "id": "c26a76adf0b703d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## What we also want to see....\n",
    "# Accuracy of glass box on easy/hard/vhard\n",
    "# Accuracy of \n",
    "#glass_box_accuracy = np.empty(shape=(50, test_grader_max_depth + 1,2, 3))       ## split idx, grader depth, train/test, easy/hard/very hard\n",
    "glass_box_accuracy[:, 4, 0, 0].mean()"
   ],
   "id": "2e0dd95ada3afccc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ln1 = ax.plot(np.arange(2, test_grader_max_depth + 1), glass_box_accuracy.mean(axis=0)[2:, TESTING_SET, 0],\n",
    "              color='k', linewidth=8, label='Accuracy (w.r.t. \"Easy\")')\n",
    "ax.set_ylabel('Accuracy', fontsize=18)\n",
    "ax.set_xlim(1, test_grader_max_depth + 1)\n",
    "ax.set_xticks(np.arange(2, test_grader_max_depth + 1), np.arange(2, test_grader_max_depth + 1))\n",
    "ax.set_xlabel('Grader Depth', fontsize=18)\n",
    "\n",
    "right_ax = ax.twinx()\n",
    "ln2 = right_ax.plot(np.arange(2, test_grader_max_depth + 1), difficulty_ratios_test[2:, 0],\n",
    "                    color='red', linewidth=8, linestyle=':', label='\"Easy\" Pattern Ratio')\n",
    "right_ax.set_ylabel('\"Easy\" Pattern Ratio', fontsize=18)\n",
    "\n",
    "lns = ln1 + ln2\n",
    "labels = [l.get_label() for l in lns]\n",
    "lg = ax.legend(lns, labels, fontsize=18, framealpha=0.95, shadow=True, loc='center right')\n",
    "lg.set_zorder(10)\n",
    "\n",
    "ax.set_title('Glass Box Accuracy vs. \"Easy\" Ratio (Testing set)', fontsize=32)\n",
    "\n",
    "plt.show()"
   ],
   "id": "b3dd965cdfd7df98",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ln1 = ax.plot(np.arange(2, test_grader_max_depth + 1), black_box_accuracy.mean(axis=0)[2:, TESTING_SET, DIFF_HARD],\n",
    "              color='k', linewidth=8, label='Accuracy (w.r.t. \"Hard\")')\n",
    "ax.set_ylabel('Accuracy', fontsize=18)\n",
    "ax.set_xlim(1, test_grader_max_depth + 1)\n",
    "ax.set_xticks(np.arange(2, test_grader_max_depth + 1), np.arange(2, test_grader_max_depth + 1))\n",
    "ax.set_xlabel('Grader Depth', fontsize=18)\n",
    "\n",
    "right_ax = ax.twinx()\n",
    "ln2 = right_ax.plot(np.arange(2, test_grader_max_depth + 1), difficulty_ratios_test[2:, DIFF_HARD],\n",
    "                    color='red', linewidth=8, linestyle=':', label='\"Hard\" Pattern Ratio')\n",
    "right_ax.set_ylabel('\"Hard\" Pattern Ratio', fontsize=18)\n",
    "\n",
    "lns = ln1 + ln2\n",
    "labels = [l.get_label() for l in lns]\n",
    "lg = ax.legend(lns, labels, fontsize=18, framealpha=0.95, shadow=True, loc='upper right')\n",
    "\n",
    "ax.set_title('Black Box Accuracy vs. \"Hard\" Ratio (Testing set)', fontsize=32)\n",
    "\n",
    "plt.show()"
   ],
   "id": "52d622430b28d12b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "x = np.linspace(0, 10, 100)\n",
    "y1 = np.sin(x)\n",
    "y2 = 100 * np.cos(x)\n",
    "\n",
    "# Create figure and first axis\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# Plot on first y-axis\n",
    "ln1 = ax1.plot(x, y1, color='b', label=\"sin(x)\")\n",
    "ax1.set_xlabel(\"X axis\")\n",
    "ax1.set_ylabel(\"sin(x)\", color='b')\n",
    "ax1.tick_params(axis='y', labelcolor='b')\n",
    "\n",
    "# Create second y-axis\n",
    "ax2 = ax1.twinx()\n",
    "ln2 = ax2.plot(x, y2, color='r', label=\"100cos(x)\")\n",
    "ax2.set_ylabel(\"100cos(x)\", color='r')\n",
    "ax2.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "# Combine legends from both axes\n",
    "lns = ln1 + ln2\n",
    "labels = [l.get_label() for l in lns]\n",
    "ax1.legend(lns, labels, loc=\"upper right\")\n",
    "\n",
    "# Optional grid\n",
    "ax1.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.title(\"Two Y-axes with Combined Legend\")\n",
    "plt.show()"
   ],
   "id": "7c34b7f15dbb5be4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ln1 = ax.plot(np.arange(2, test_grader_max_depth + 1), black_box_accuracy.mean(axis=0)[2:, TESTING_SET, 1],\n",
    "              color='k', linewidth=8, label='bb accuracy hard')\n",
    "ln22 = ax.plot(np.arange(2, test_grader_max_depth + 1), black_box_accuracy.mean(axis=0)[2:, TESTING_SET, 2],\n",
    "              color='b', linewidth=8, label='bb accuracy v_hard')\n",
    "ax.set_ylabel('Accuracy', fontsize=18)\n",
    "ax.set_xlim(1, test_grader_max_depth + 1)\n",
    "ax.set_xticks(np.arange(2, test_grader_max_depth + 1), np.arange(2, test_grader_max_depth + 1))\n",
    "ax.set_xlabel('Grader Depth', fontsize=18)\n",
    "\n",
    "right_ax = ax.twinx()\n",
    "ln2 = right_ax.plot(np.arange(2, test_grader_max_depth + 1), difficulty_ratios_test[2:, 2],\n",
    "                    color='red', linewidth=8, linestyle=':', label='\"Easy\" Pattern Ratio')\n",
    "right_ax.set_ylabel('\"Easy\" Pattern Ratio', fontsize=18)\n",
    "\n",
    "lns = ln1 + ln22 + ln2\n",
    "labels = [l.get_label() for l in lns]\n",
    "lg = ax.legend(lns, labels, fontsize=18, framealpha=0.95, shadow=True, loc='center right')\n",
    "lg.set_zorder(10)\n",
    "\n",
    "ax.set_title('Accuracy on Rejected Patterns', fontsize=32)\n",
    "\n",
    "plt.show()"
   ],
   "id": "f2a657d18b6b3007",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "23c963f4d68840b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2ef6557aa8741ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "594d79777da123c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a14f5163a2b77c42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "bfa064db7ab51898",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "92f164e0045af97a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b0d5d1dd785ed858",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "41e7389b4a2c83c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a1b33f4230976b7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b4df935cb44d11d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "bff2b261cdfa2957",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "539a90b13328f842",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2d3949634b996d52",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "23471b53b224d68e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e811971f1c8557d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c7253274216abf22",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f0afa9934212913f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1f03877f19cea1eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e09c86ad2eef5afa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2f3dd28d7596bf09",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ef17c87b8e4786e9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
